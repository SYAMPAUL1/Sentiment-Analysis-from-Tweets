{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65531b1",
   "metadata": {},
   "source": [
    "Student Name: SYAM IMMANUEL PAUL BONDADA\n",
    "\n",
    "Student Number: 230853737"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3049c",
   "metadata": {},
   "source": [
    "# Questions 5: Optimising pre-processing and feature extraction (30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c336c21",
   "metadata": {},
   "source": [
    "**Note:** it is advisable to implement question 5 in a separate notebook where you further develop the pre-processing and feature extraction functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3fafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = False  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3945e29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 33540 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 33540 rawData, 26832 trainData, 6708 testData\n",
      "Training Samples: \n",
      "26832\n",
      "Features: \n",
      "343532\n",
      "Training Classifier...\n",
      "Fold 0: {'precision': 0.8730287257085559, 'recall': 0.8748137108792846, 'f1-score': 0.8718090506056304, 'accuracy': 0.8748137108792846}\n",
      "Training Classifier...\n",
      "Fold 2684: {'precision': 0.8744147065359611, 'recall': 0.8789120715350224, 'f1-score': 0.8754439785887319, 'accuracy': 0.8789120715350224}\n",
      "Training Classifier...\n",
      "Fold 5368: {'precision': 0.8598334761154659, 'recall': 0.8602831594634873, 'f1-score': 0.8599925727485419, 'accuracy': 0.8602831594634873}\n",
      "Training Classifier...\n",
      "Fold 8052: {'precision': 0.8814588913525726, 'recall': 0.8800298062593145, 'f1-score': 0.8798784797099655, 'accuracy': 0.8800298062593145}\n",
      "Training Classifier...\n",
      "Fold 10736: {'precision': 0.8656530098904243, 'recall': 0.86698956780924, 'f1-score': 0.8652064744365849, 'accuracy': 0.86698956780924}\n",
      "Training Classifier...\n",
      "Fold 13420: {'precision': 0.8752814630804957, 'recall': 0.8763040238450075, 'f1-score': 0.8747899028463306, 'accuracy': 0.8763040238450075}\n",
      "Training Classifier...\n",
      "Fold 16104: {'precision': 0.8803921519339208, 'recall': 0.8815201192250373, 'f1-score': 0.8805279446287796, 'accuracy': 0.8815201192250373}\n",
      "Training Classifier...\n",
      "Fold 18788: {'precision': 0.8736343195447254, 'recall': 0.8748137108792846, 'f1-score': 0.8730106521197435, 'accuracy': 0.8748137108792846}\n",
      "Training Classifier...\n",
      "Fold 21472: {'precision': 0.8743260338969931, 'recall': 0.875558867362146, 'f1-score': 0.8740701597469168, 'accuracy': 0.875558867362146}\n",
      "Training Classifier...\n",
      "Fold 24156: {'precision': 0.8801126229474835, 'recall': 0.8807922272047832, 'f1-score': 0.8790404825421733, 'accuracy': 0.8807922272047832}\n",
      "Cross-validation results: {'precision': 0.87381354010066, 'recall': 0.8750017264462606, 'f1-score': 0.8733769697973399, 'accuracy': 0.8750017264462606}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage, global_feature_dict, ngram_range=(1, 1), top_n_features=None):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and perform the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    \n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text), global_feature_dict, ngram_range, top_n_features), label))\n",
    "\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text), global_feature_dict, ngram_range, top_n_features), label))\n",
    "\n",
    "def parse_data_line(data_line):\n",
    "    label = data_line[1]\n",
    "    text = data_line[2]\n",
    "    pre_process(text)\n",
    "    return label, text\n",
    "\n",
    "def pre_process(text):\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    custom_stopwords = set([\"list\", \"of\", \"custom\", \"stopwords\"])\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in custom_stopwords]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def load_opinion_lexicon():\n",
    "    \"\"\"Load opinion lexicon from file.\"\"\"\n",
    "    positive_words = set()\n",
    "    negative_words = set()\n",
    "\n",
    "    with open('positive-words.txt', 'r', encoding='latin-1') as positive_file:\n",
    "        positive_words = set(positive_file.read().splitlines())\n",
    "\n",
    "    with open('negative-words.txt', 'r', encoding='latin-1') as negative_file:\n",
    "        negative_words = set(negative_file.read().splitlines())\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "def lexicon_feature(tokens, positive_words, negative_words):\n",
    "    \"\"\"Extract lexicon-based features and stylistic features.\"\"\"\n",
    "    num_positive = sum(1 for token in tokens if token in positive_words)\n",
    "    num_negative = sum(1 for token in tokens if token in negative_words)\n",
    "\n",
    "    # Stylistic feature: Average number of words per sentence\n",
    "    num_sentences = len(re.split(r'[.!?]', ' '.join(tokens)))\n",
    "    avg_words_per_sentence = len(tokens) / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    return {'num_positive': num_positive, 'num_negative': num_negative, 'avg_words_per_sentence': avg_words_per_sentence}\n",
    "\n",
    "def to_feature_vector(tokens, global_feature_dict, ngram_range=(1, 1), top_n_features=None):\n",
    "    positive_words, negative_words = load_opinion_lexicon()  # Load opinion lexicon\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=top_n_features)\n",
    "    \n",
    "    feature_vector = vectorizer.fit_transform([\" \".join(tokens)])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for feature_name in feature_names:\n",
    "        global_feature_dict[feature_name] = global_feature_dict.get(feature_name, 0) + 1\n",
    "\n",
    "    lexicon_features = lexicon_feature(tokens, positive_words, negative_words)\n",
    "    feature_dict = dict(zip(feature_names, feature_vector.toarray()[0]))\n",
    "    feature_dict.update(lexicon_features)\n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline = Pipeline([('svc', LinearSVC(dual=False, max_iter=10000))])\n",
    "    return SklearnClassifier(pipeline).train(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grid_search_svm_c(train_data, svm_c_values):\n",
    "    \"\"\"Perform grid search for SVM cost parameter (C).\"\"\"\n",
    "    param_grid = {'svc__C': svm_c_values}\n",
    "    pipeline = Pipeline([('svc', LinearSVC())])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='f1_weighted')\n",
    "    grid_search.fit([sample[0] for sample in train_data], [sample[1] for sample in train_data])\n",
    "\n",
    "    best_c = grid_search.best_params_['svc__C']\n",
    "\n",
    "    return best_c\n",
    "\n",
    "def cross_validate(dataset, folds, top_n_features=None, svm_c_values=None):\n",
    "    results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1-score': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "\n",
    "    fold_size = int(len(dataset) / folds) + 1\n",
    "\n",
    "    for i in range(0, len(dataset), fold_size):\n",
    "        test_data_fold = dataset[i:i + fold_size]\n",
    "        train_data_fold = dataset[:i] + dataset[i + fold_size:]\n",
    "\n",
    "        # Train the classifier\n",
    "        if svm_c_values is not None:\n",
    "            best_c = grid_search_svm_c(train_data_fold, svm_c_values)\n",
    "            print(f\"Best SVM Cost Parameter (C) for Fold {i}: {best_c}\")\n",
    "\n",
    "        classifier = train_classifier(train_data_fold)\n",
    "\n",
    "        # Test the classifier\n",
    "        test_samples, true_labels = zip(*test_data_fold)\n",
    "        predicted_labels = [classifier.classify(sample) for sample in test_samples]  # Use classify method\n",
    "\n",
    "        # Evaluate the classifier\n",
    "        report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "\n",
    "        # Store and print results for each fold\n",
    "        fold_results = {\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1-score': report['weighted avg']['f1-score'],\n",
    "            'accuracy': report['accuracy']\n",
    "        }\n",
    "\n",
    "        print(f\"Fold {i}: {fold_results}\")\n",
    "\n",
    "        results['precision'].append(fold_results['precision'])\n",
    "        results['recall'].append(fold_results['recall'])\n",
    "        results['f1-score'].append(fold_results['f1-score'])\n",
    "        results['accuracy'].append(fold_results['accuracy'])\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_results = {\n",
    "        'precision': np.mean(results['precision']),\n",
    "        'recall': np.mean(results['recall']),\n",
    "        'f1-score': np.mean(results['f1-score']),\n",
    "        'accuracy': np.mean(results['accuracy'])\n",
    "    }\n",
    "    return avg_results\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # references to the data files\n",
    "    data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "    # Do the actual stuff (i.e. call the functions we've made)\n",
    "    # We parse the dataset and put it in a raw data list\n",
    "    print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "          \"Preparing the dataset...\",sep='\\n')\n",
    "    \n",
    "    load_data(data_file_path)\n",
    "\n",
    "    # We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "    # You do the cross-validation on the 80% (training data)\n",
    "    # We print the number of training samples and the number of features before the split\n",
    "    print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "          \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "    global_feature_dict = {}\n",
    "    split_and_preprocess_data(0.8, global_feature_dict, ngram_range=(1, 2), top_n_features=5000)\n",
    "\n",
    "    # We print the number of training samples and the number of features after the split\n",
    "    print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "          \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n",
    "\n",
    "    cv_results = cross_validate(train_data, folds=10)\n",
    "    print(\"Cross-validation results:\", cv_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_data = []          # the filtered data from the dataset file\n",
    "    train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "    test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7ad4d",
   "metadata": {},
   "source": [
    "Improvements In pre-processing: \n",
    "\n",
    "1. Tokenization:\n",
    "  Here in the improved version I used re.split(r\"\\s+\", text) for tokenization, which is a simpler and more straightforward approach than word_tokenize from NLTK. It splits the text based on whitespace. \n",
    "\n",
    "2. Custom Stopwords: \n",
    "  The custom_stopwords set consists of user-defined stopwords, which, in this example, includes common words such as \"list,\" \"of,\" \"custom,\" and \"stopwords.\" The purpose of this set is to identify and exclude these specific words during the text pre-processing stage.\n",
    "\n",
    "3. Punctuation Handling:  \n",
    "  These punctuation handling steps contribute to the overall preprocessing of the text data. Separating punctuation from words helps ensure that each token (word) is treated as an individual entity during subsequent processing steps, such as lowercase conversion and lemmatization. This can be beneficial for sentiment analysis tasks as it allows the model to focus on the semantic content of words while disregarding attached punctuation marks. \n",
    "\n",
    "4. Normalization: \n",
    "\n",
    "  Lowercasing: The line tokens = [t.lower() for t in tokens if t.lower() not in custom_stopwords] ensures that all words in the text are converted to lowercase. Lowercasing helps standardize the text by treating uppercase and lowercase versions of the same word as identical. This is essential for consistency in subsequent analyses. \n",
    "Lemmatization: The lemmatization step is performed using the WordNetLemmatizer from the NLTK library: tokens = [lemmatizer.lemmatize(t) for t in tokens]. Lemmatization involves reducing words to their base or root form. For example, lemmatizing \"running\" would result in \"run.\" This step aims to ensure that different inflections or derivations of a word are treated as the same, reducing the dimensionality of the feature space.\n",
    "\n",
    "Lexicon-based features:\n",
    "Positive words: The presence of positive words in the text suggests a positive sentiment. The more positive words there are, the more likely the text is to express a positive sentiment.\n",
    "Negative words: The presence of negative words in the text suggests a negative sentiment. The more negative words there are, the more likely the text is to express a negative sentiment.\n",
    "\n",
    "By combining information about positive and negative words, lexicon-based features can provide a more accurate assessment of the sentiment expressed in the text. When combined with normal feature extraction techniques, these features can help to improve the overall accuracy of sentiment analysis models.\n",
    "\n",
    "Stylistic features:\n",
    "Average number of words per sentence: A higher average number of words per sentence suggests a more formal or analytical style of writing. This style is often associated with more neutral or objective sentiment.\n",
    "Use of exclamation points: The use of exclamation points suggests a more emotional or emphatic style of writing. This style is often associated with more positive or negative sentiment.\n",
    "\n",
    "By analyzing stylistic features, the model can gain insights into the writer's intent and the overall emotional tone of the text. This information can be used to improve the accuracy of the sentiment analysis.\n",
    "\n",
    "\n",
    "Grid search and parameter optimization techniques like it are crucial for improving the accuracy of machine learning models, including support vector machines (SVMs). Compared to using a fixed or default parameter value, grid search helps to identify the optimal parameter settings that lead to the best performance on a given dataset.\n",
    "\n",
    "The second approach, which involves cross-validation, demonstrates a more stable and higher overall performance, as indicated by the consistent accuracy and elevated F1-score. This suggests that the model trained in the second case generalizes well across different subsets of the data, providing more reliable and robust predictions compared to the individual fold-wise evaluations in the first case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd01d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
